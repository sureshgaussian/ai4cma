{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob(\"../data/training/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json_files[0: int(num_inputs*0.7) ]\n",
    "test = json_files[int(num_inputs*0.7): num_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/training/AK_Valdez.json',\n",
       " '../data/training/NV_Tonopah_321370_1961_62500_geo_mosaic.json',\n",
       " '../data/training/CO_Silverton_451014_1955_24000_geo_mosaic.json',\n",
       " '../data/training/WI_StevensPoint_700274_1956_48000_geo_mosaic.json',\n",
       " '../data/training/ID_Bellevue_238908_1957_62500_geo_mosaic.json',\n",
       " '../data/training/WI_Wyeville_503636_1958_48000_geo_mosaic.json',\n",
       " '../data/training/CO_GrandJunction.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/training/IN_ClayCity_160547_1908_48000_geo_mosaic.json',\n",
       " '../data/training/NV_SanAntonioRanch_321264_1964_62500_geo_mosaic.json',\n",
       " '../data/training/NM_TetillaPeak.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/training/AK_Valdez.json',\n",
       " '../data/training/NV_Tonopah_321370_1961_62500_geo_mosaic.json',\n",
       " '../data/training/CO_Silverton_451014_1955_24000_geo_mosaic.json',\n",
       " '../data/training/WI_StevensPoint_700274_1956_48000_geo_mosaic.json',\n",
       " '../data/training/ID_Bellevue_238908_1957_62500_geo_mosaic.json',\n",
       " '../data/training/WI_Wyeville_503636_1958_48000_geo_mosaic.json',\n",
       " '../data/training/CO_GrandJunction.json',\n",
       " '../data/training/IN_ClayCity_160547_1908_48000_geo_mosaic.json',\n",
       " '../data/training/NV_SanAntonioRanch_321264_1964_62500_geo_mosaic.json',\n",
       " '../data/training/NM_TetillaPeak.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for json_file in json_files:\n",
    "    with open(json_file) as jfile:\n",
    "        label_info = json.load(jfile)\n",
    "\n",
    "    img_ht = label_info[\"imageHeight\"]\n",
    "    img_wd = label_info[\"imageWidth\"]\n",
    "\n",
    "    inp_file = json_file.replace(\".json\", \".tif\")\n",
    "    #print(inp_file)\n",
    "\n",
    "    for shape in label_info[\"shapes\"]:\n",
    "        label = shape[\"label\"]\n",
    "        label_type = label.split(\"_\")[-1]\n",
    "        label_file = inp_file.replace(\".tif\", \"\")+\"_\"+label+\".tif\"\n",
    "        points = shape[\"points\"]\n",
    "        inputs.append( [json_file, img_ht, img_wd, inp_file, label, label_type, label_file, points] )\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(inputs, columns=[\"json_file\", \"height\", \"width\", \"inp_file\", \"label\", \"label_type\", \"label_file\", \"points\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "poly    3672\n",
       "pt       393\n",
       "line     197\n",
       "Name: label_type, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_files = df.loc[df.label_type == \"poly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_files.to_csv(\"poly_input_info.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files = df.loc[df.label_type == \"pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_files.to_csv(\"pt_input_info.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_files = df.loc[df.label_type == \"line\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_files.to_csv(\"line_input_info.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_input_info.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "short = pd.concat([ poly_files.iloc[0:10], line_files.iloc[0:3], pt_files.iloc[0:2] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "short.to_csv(\"short_input_info.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd \n",
    "import json \n",
    "import os \n",
    "def prepare_input_info_csv(input_dir, info_file=\"input_info.csv\"):\n",
    "    json_files = glob.glob(os.path.join(input_dir, \"*.json\"))\n",
    "    inputs = []\n",
    "    for json_file in json_files:\n",
    "        with open(json_file) as jfile:\n",
    "            label_info = json.load(jfile)\n",
    "\n",
    "        img_ht = label_info[\"imageHeight\"]\n",
    "        img_wd = label_info[\"imageWidth\"]\n",
    "\n",
    "        inp_file = json_file.replace(\".json\", \".tif\")\n",
    "        #print(inp_file)\n",
    "\n",
    "        for shape in label_info[\"shapes\"]:\n",
    "            label = shape[\"label\"]\n",
    "            label_type = label.split(\"_\")[-1]\n",
    "            label_file = inp_file.replace(\".tif\", \"\")+\"_\"+label+\".tif\"\n",
    "            points = shape[\"points\"]\n",
    "            inputs.append( [json_file, img_ht, img_wd, inp_file, label, label_type, label_file, points] )\n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(inputs, columns=[\"json_file\", \"height\", \"width\", \"inp_file\", \"label\", \"label_type\", \"label_file\", \"points\"])\n",
    "    df.to_csv(info_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_input_info_csv(\"../data/training/\", \"inp_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.read_csv(\"../eda/short_input_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_path = \"../data/short_inp/\"\n",
    "if not os.path.isdir(copy_path):\n",
    "    os.mkdir(\"../data/short_inp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# lets first copy all input files\n",
    "in_files = set(sdf.inp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inf in in_files:\n",
    "    shutil.copy(inf, os.path.join(copy_path, os.path.basename(inf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets copy the json files\n",
    "json_files = set(sdf.json_file)\n",
    "for inf in json_files:\n",
    "    shutil.copy(inf, os.path.join(copy_path, os.path.basename(inf)))\n",
    "\n",
    "# lets copy label files\n",
    "label_files = set(sdf.label_file)\n",
    "for inf in label_files:\n",
    "    shutil.copy(inf, os.path.join(copy_path, os.path.basename(inf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_input_dir = \"../data/short_inp/tiled_inputs\"\n",
    "tiled_label_dir = \"../data/short_inp/tiled_labels\"\n",
    "tiled_mask_dir = \"../data/short_inp/tiled_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile the inputs, labels, masks\n",
    "\n",
    "import img2tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 256\n",
    "for inf in in_files:\n",
    "    in_tiles = img2tiles.split_image_into_tiles(inf, tiled_input_dir, tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inf in label_files:\n",
    "    label_mask_tiles = img2tiles.split_image_into_tiles(inf, tiled_mask_dir, tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "inputs_descriptor = []\n",
    "for idx, row in sdf.iterrows():\n",
    "    in_tiles = img2tiles.split_image_into_tiles(row.inp_file, tiled_input_dir, tile_size)\n",
    "\n",
    "    points = ast.literal_eval(row.points)\n",
    "    label_mask_tiles = img2tiles.split_image_into_tiles(row.label_file, tiled_mask_dir, tile_size)\n",
    "    label_pattern_fname = img2tiles.make_label_pattern(row.inp_file, row.label, points, tiled_label_dir, tile_size)\n",
    "\n",
    "    assert(len(label_mask_tiles) == len(in_tiles))\n",
    "    for tile_no, tile in enumerate(label_mask_tiles):\n",
    "        empty_tile = img2tiles.check_non_zero_tile(os.path.join(tiled_mask_dir, label_mask_tiles[tile_no]))\n",
    "        inputs_descriptor.append([row.inp_file, row.height, row.width, in_tiles[tile_no], label_pattern_fname, label_mask_tiles[tile_no], empty_tile, tile_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(inputs_descriptor, columns = [\"orig_file\", \"orig_ht\", \"orig_wd\", \"tile_inp\", \"tile_legend\", \"tile_mask\", \"empty_tile\", \"tile_size\"])\n",
    "df.to_csv(\"short_input.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_true of the dataset = 0.016696022310289153\n",
      "length of true = 455\n",
      "length of true_train = 364\n",
      "length of true_test = 91\n",
      "0.027826703850481922\n",
      "Avg_True  0.37877211238293446\n",
      "Avg_True test 0.37916666666666665\n",
      "train len: 961, test len: 240\n"
     ]
    }
   ],
   "source": [
    "prepare.prepare_balanced_inputs(\"short_input.csv\", \"short_train.csv\", \"short_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ai4cma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3db3ed26f3aa86112f8cf54069cb585daa83d22f4633c0253ccad5ec9f75677"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
