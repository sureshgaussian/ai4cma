Inputs on October 9th
------------
1. code/training_input_info.csv: the entire training data set provided by ai4cma challenge info captured
2. eda/train_split.csv, eda/test_split.csv : input files based train, test split (80%,20%) of training_input_info.csv 
3. prepare.py : use this file to generate individual tiles necessary for training. (the process is currently running)
4. prepare.py will generate two files:
    a. training_tiled_inputs.csv: eda/train_split.csv json files, processed to make input files, label_files, and mask_files (97% empty tiles) [200K]
    b. balanced_tiled_training.csv : balanced training inputs to get empty tiles to be ~ 60% only. you can adjust the ratio in prepare.py [4K]
5. use balanced_tiled_training.csv for training.. save the model (lets say ai4cma_v1.tar)
5.a: to get the test_split tiled files:
    i) change line 220 in prepare.py to ../eda/test_split.csv 
    ii) line 229 to test_tiled_inputs.csv
    iii) changed 233 to balanced_tiled_tests.csv 
6. For inference:
    a) mkdir 'test_set'
    b) copy the files that appear in eda/test_split.csv (inp_fname) to the above folder (better way is to change inference.py to take a csv as input, may be will do next)
    c) set the directories and config needed for inference
    d) run inference.py

During iference, write individual scores for each legend to a csv for better comparison

1. Try other losses: dice, distance penalized cross entropy loss, focal loss 
2. fix save segments functions
3. change legends

First focus on validation:
----------------------
- Inputs are in data/validation folder. contains json, and base tif files.
- tile_size = 256
- step 1: for each label, produce tiled_inputs, tiled_legend, ht, wd, legend_type (pass on the non-poly type)
- step 2: pass predict.csv to predict.

Ravi: Check the baseline points & lines f1 score
Suresh: Check the baseline polygons f1 score 
Ravi: try SIFT like algos on points 

